---
title: "Nonlinear mixed effects models for time series forecasting of smart meter demand"
author:
- familyname: Roach
  othernames: Cameron
  address: Monash University
  email: cameron.roach@monash.edu
  correspondingauthor: true
- familyname: Hyndman
  othernames: Rob
  address: Monash University
- familyname: Taieb
  othernames: Souhaib Ben
  address: University of Mons
abstract: Buildings are typically equipped with smart meters to measure electricity demand at regular intervals. Smart meter data for a single building have many uses, such as forecasting and assessing overall building performance. However, when data are available from multiple buildings, there are additional applications that are rarely explored. For instance, we can explore how different building characteristics influence energy demand. If each building is treated as a random effect and building characteristics are handled as fixed effects, a mixed effects model can be used to estimate how characteristics affect energy usage. In this paper we demonstrate that producing one-day ahead demand predictions for 123 commercial office buildings using mixed models can improve forecasting accuracy. We experiment with random intercept, random intercept and slope, and nonlinear mixed models. The predictive performance of the mixed effects models are tested against naive, linear and nonlinear benchmark models fitted to each building separately. This research justifies using mixed models to improve forecasting accuracy and to quantify changes in energy consumption under different building configuration scenarios.
keywords: "time series forecasting, mixed-effects models, smart meters, energy, electricity"
wpnumber: 1/19
jelcodes: C10,C14,C52
blind: false
cover: false
toc: false
bibliography: ["library.bib", "packages.bib"]
biblio-style: authoryear-comp
output:
  MonashEBSTemplates::workingpaper:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  cache.lazy = FALSE,
  dev = "pdf",
  fig.height = 5,
  fig.width = 8.5
)
library(tidyverse)
library(tsibble)
library(furrr)
library(lubridate)
library(stringr)
library(lme4)
# library(nlme)
library(splines)
library(forecast)
library(leaps)
source("R/load-data.R")
source("R/feature-selection.R")
source("R/test-models.R")
source("R/models-lme4.R")
source("R/plot-helpers.R")
```

```{r init}
# packages .bib file
knitr::write_bib(c("base", "lme4"),
  "packages.bib",
  width = 60
)


set.seed(123412)

# furrr/future settings
options(future.globals.maxSize = 850 * 1024^2 ,  # increase limit to 850 Mb
        future.fork.enable = TRUE)  # disabled by default, but I haven't noticed stability issues
plan(multicore, workers = availableCores()-1) # multicore forks R processes and requires future.fork.enable = TRUE as above

# The base::scale function returns a matrix which causes issues later on when
# fitting and predicting with lm. Use this because it returns a vector.
scale_dbl <- function(x) {
  (x - mean(x)) / sd(x)
}
```

```{r parameters}
#### Parameters ===============================================================
# Data
data_dir <- "/mnt/Documents/data/building_level"
qh_path <- file.path(data_dir, "qh_2018")
outlier_file <- file.path(data_dir, "outlier_dates_20181120.csv")
attribute_file <- file.path(data_dir, "attributes_20181019_tidy.csv")
building_list_file <- file.path(data_dir, "building_list_20181022.csv")

all_data_file <- "cache/main_df.RData"
fs_df_file <- "cache/fs_df.RData"
model_df_file <- "cache/model_df.RData"
accuracy_df_file <- "cache/accuracy_df.RData"

# Modelling
n_max_features <- NULL
fcst_test_periods <- 1:96
fcst_test_dates <- dmy("3/1/2017") + 0:355 # exclude final week of year and first day back due to holiday effects
fcst_train_window <- 120
fcst_min_train_days <- 40
building_attributes <- c(
  "basebldngfeedonly",
  "dxsystem",
  "electricelementheating",
  "centraldist"
)
predictor_vars <- read_csv("data/predictor_vars.csv")
formula_all <- as.formula(paste("~ bid +", get_terms(predictor_vars$vars)))

# Presentation
p_one_day_ahead_date <- dmy("23/8/2017")
scenario_bid <- "BID0010"
scenario_dates <- ymd("2017-01-09") + 0:4
business_hour_periods <- 29:76
```

```{r main}
#### Setup ====================================================================
# Run if qh .RData file hasn't been created
if (!file.exists(all_data_file)) {
  message("Processing data...\n\n")
  main_df <- load_all_data(
    attribute_file = attribute_file,
    qh_path = qh_path,
    building_list_file = building_list_file,
    business_days = TRUE
  )

  # Scale independent variables
  main_df <- main_df %>%
    rename_at(vars(starts_with("temperature")), ~ paste0("scaled_", .)) %>%
    rename_at(vars(starts_with("wh_lag")), ~ paste0("scaled_", .)) %>%
    mutate(
      temperature = scaled_temperature, # keep original temperature for plotting
      wh_lag_96 = scaled_wh_lag_96      # keep original 96 lag demand for plotting
    ) %>%
    mutate_at(vars(starts_with("scaled")), scale_dbl) %>% # scale
    select(
      ts, date, period, bid, wh, temperature, wh_lag_96,
      starts_with("scaled"), !!building_attributes
    )

  # Filter for training and test dates only
  dates_filter <- fcst_test_dates %>%
    map(~ .x - days(0:fcst_train_window)) %>%
    as_vector() %>%
    as_date() %>%
    unique()

  main_df <- main_df %>%
    filter(
      date %in% dates_filter,
      period %in% fcst_test_periods
    )

  save(main_df, file = all_data_file)
} else {
  message("Loaded cached processed data...\n\n")
  load(all_data_file)
}

# Calculate other variables
n_buildings <- length(unique(main_df$bid))
fcst_test_dates <- as_date(intersect(fcst_test_dates, unique(main_df$date)))
```


```{r features}
#### Feature selection ========================================================
if (!file.exists(fs_df_file)) {
  message(
    "Running feature selection for max of", n_max_features,
    "candidate predictors...\n\n"
  )
  start_time <- Sys.time()
  fs_df <- list(
    month = unique(month(fcst_test_dates)),
    period = fcst_test_periods
  ) %>%
    cross_df() %>%
    mutate(fs_date = map(
      month,
      ~ min(fcst_test_dates[month(fcst_test_dates) == .x])
    )) %>%
    unnest_legacy() %>%
    mutate(features = future_map2(
      fs_date,
      period,
      select_features,
      formula = formula_all,
      train_window = fcst_train_window,
      min_train_days = fcst_min_train_days,
      n_var_max = n_max_features,
      .progress = TRUE
    ))

  print(Sys.time() - start_time)
  save(fs_df, file = fs_df_file)
} else {
  message("Loaded cached feature selection file...\n\n")
  load(fs_df_file)
}

fs_df <- fs_df %>%
  mutate(vars = map(features, ~ .x %>%
    filter(loocv == min(loocv)) %>%
    pull(vars) %>%
    unlist()))
```


```{r models}
#### Fit models ===============================================================
if (!file.exists(model_df_file)) {
  message("Fitting models...\n\n")
  start_time <- Sys.time()

  model_df <- list(
    period = fcst_test_periods,
    fcst_date = fcst_test_dates
  ) %>%
    cross_df() %>%
    mutate(fcst_date = as_date(fcst_date)) %>%
    mutate(fit_results = future_map2(
      fcst_date,
      period,
      test_models,
      scenario_dates = scenario_dates,
      train_window = fcst_train_window,
      min_train_days = fcst_min_train_days,
      .progress = TRUE
    )) %>%
    unnest_legacy(fit_results)

  print(Sys.time() - start_time)
  save(model_df, file = model_df_file)
} else {
  message("Loaded cached models...\n\n")
  load(model_df_file)
  fcst_test_dates <- unique(model_df$fcst_date) # use what is in cached file
}
```

```{r accuracy}
#### Calculate accuracy =======================================================
if (!file.exists(accuracy_df_file)) {
  message("Calculating accuracy scores...\n\n")

  calculate_accuracy <- function(df, periods) {
    df <- df %>%
      unnest_legacy(preds) %>%
      filter(period %in% periods) %>%
      group_by(model)

    mase_df <- df %>%
      summarise(mae = mean(abs(resid)))

    naive_mae <- mase_df %>%
      filter(model == "Naive") %>%
      pull(mae)

    mase_df <- mase_df %>%
      mutate(mase = mae / naive_mae)

    accuracy_df <- df %>%
      summarise(
        mape = 100 * mean(abs(resid / wh)),
        smape = 200 * mean(abs(resid) / (pred + wh))
      ) %>%
      inner_join(mase_df, by = "model")

    accuracy_df
  }

  accuracy_df <- tribble(
    ~working_hours, ~periods,
    "All hours", 1:96,
    "Business hours", business_hour_periods,
    "Non-business hours", setdiff(1:96, business_hour_periods)
  ) %>%
    mutate(accuracy = map(periods, ~ calculate_accuracy(model_df, .x))) %>%
    unnest_legacy(accuracy)

  save(accuracy_df, file = accuracy_df_file)
} else {
  message("Loaded accuracy data frame...\n\n")
  load(accuracy_df_file)
}
```



# Introduction

Several papers have examined forecasting electricity demand for buildings by fitting separate models to each building [@Ghofrani2011-tb; @Gajowniczek2014-ek; @Arora2016-zh; @Ben_Taieb2016-wl]. While some have attempted to improve forecasts by leveraging the hierarchical nature of electricity demand [@Ben_Taieb2020-it; @Ben_Taieb2017-ok] few, if any, have explored improving forecast accuracy using a mixed effects framework. If buildings behave in a similar manner a well-specified mixed model may produce more accurate forecasts than individual models. Furthermore, a mixed-model framework allows us to quantify differences between buildings which would not otherwise be possible when using a "building-specific" modelling approach. A mixed effects approach opens the door to scenario analyses by allowing us to estimate how demand might change under different equipment or usage scenarios.

This paper explores how electricity forecasting accuracy can be improved by using mixed effects models. We examine if mixed models can produce forecasts as accurately as separate models fit for each subject. We approach the problem in the context of producing one-day ahead forecasts of electricity demand for `r n_buildings` commercial office buildings in Australia. When working with mixed effects models, each building is treated as a random effect and building characteristics are treated as fixed effects. We attempt to model the relationship between temperature and demand using both linear and spline based methods.

To the author's knowledge few papers have explored using mixed models in an electricity demand forecasting role. @Brabec2008-jf appears to be closest to this area. In their paper, a nonlinear mixed effects model (NLME) was used to forecast daily gas demand for individual customers. Predictors such as day of week and temperature were treated as random effects. Their NLME model was benchmarked against ARIMAX and ARX approaches. The paper concluded by saying there was no clear winner between the NLME and benchmark models and that both potentially have strengths and weaknesses. Unfortunately, there are few other papers within the energy field that use mixed effects models^[Some papers claim to use mixed models. However, this term is often applied to cases where a combination of models have been used which is different to mixed effects models in the statistical sense.] for forecasting.

Moving away from the energy sector there are more papers to draw from. @Ibrahim2013-oc compared the performance of fixed effects and mixed effects models when forecasting call center arrivals. Making use of correlation structures within the data was shown to improve forecast accuracy when tested against several benchmark models on real-world data sets. @Frees2004-sx explored lottery sales forecasting by postcode using a linear mixed model applied to longitudinal data. They derived best linear unbiased predictors for what they termed longitudinal data mixed models. Random effects were incorporated for each subject and, separately, each time period. When compared against an ordinary regression model (with common intercept between all subjects) and a basic fixed effects model (with a different intercept for each subject), both with $AR(1)$ error structures, the mixed model that used both time and subject random effects (two-way error model) was found to be inferior when forecasting on an out-of-sample test set. However, another one-way error components model that only included treated subjects as random effects was found to produce the best forecasts overall. This suggests that mixed models can compete with ordinary _pooled_ regression models. However, the question remains as to how well a mixed model would perform when compared to ordinary regression models fit _separately_ to each subject. Another paper that focused on call center forecasting [@Aldor-Noiman2009-ji] used a mixed Poisson process to estimate future arrival counts. @Soyer2008-in had a similar aim and showed that a Bayesian approach incorporating random effects was superior to a fixed effects model. These papers all point to the viability of using mixed effects models for forecasting.

Few papers have attempted to assess the impact of differences in building characteristics using statistical methods and smart meter data. To the author's knowledge, only a previous paper by @Roach2020-ch has looked into this using mixed effects models. Whereas that paper focused on estimating demand impact profiles for building attributes at different times of the year, this paper focuses on improving forecast accuracy for buildings with different characteristics.

Several papers have shown the relationship between electricity demand and temperature are well modelled using nonparametric components such as cubic splines [@Hyndman2010-ui; @Fan2012-bs]. This paper uses a similar approach within a mixed model framework. Other papers that explore semiparametric mixed models include @Grajeda2016-vr; @Ugarte2009-hx; and @Durban2005-lk. @Durban2005-lk is of particular note as it introduces the concept of subject-specific curves using piecewise linear splines for longitudinal data. We build on the idea of subject-specific curves by applying them to time series data and incorporating natural splines.

The main contribution of this paper is to present an approach to forecasting electricity demand for individual buildings using a mixed effects framework. Furthermore, our methodology is tested against several other benchmark models to quantify how forecasting accuracy is improved. Finally, this paper serves to enrich the literature on forecasting with mixed effects models and smart meter data.

The paper is structured as follows. Section \@ref(data) describes the data we are working with. Section \@ref(methodology) gives a detailed description of the models formulations and how they are assessed. Forecasting results are presented in Section \@ref(results). Concluding remarks are given in Section \@ref(conclusion).

# Data

We have time series and attribute data for `r n_buildings` commercial office buildings located across Australia. We focus on business days in our analysis as these are significantly more important than non-business days for energy management. Non-business days typically have far less demand than business days as equipment is non-operational. Note that our approach can be applied to non-business days as well.

## Time series data

Smart meter data recorded at 15-minute intervals for `r n_buildings` buildings are used when training and validating our models. The electricity demand is normalised by each building's net lettable area (NLA) to ensure demand is comparable between buildings. An example of a day of smart meter readings from six buildings is shown in Figure \@ref(fig:wh-line). Temperature data recorded at 15-minute intervals from the closest available weather station are also available for each building.

The relationship between current temperature and electricity demand is shown in Figure \@ref(fig:temp-wh-scatter) for two buildings at midday and midnight. There is a clear difference in this relationship between the business and non-business periods. Furthermore, this relationship varies between each building. Note that when splines are fitted independently for each building they can be very sensitive to outliers or data at domain extremes. This partly motivates our use of mixed effects models where data from multiple buildings can be used to fit these relationships resulting in models that are more robust to outliers or sparse data at extremes.

One issue that arises from only including business days is the gap between Friday and Monday in our demand time series. As we are including one-day lagged demand as one of our predictors, observed values on Monday will use Friday demand values as their one-day lagged demand (and similarly for two-day lagged demand variables). If this weren't done and we instead used non-working days, a very different relationship between Monday's lagged demand variables and current demand would be observed compared to other weekdays.

Unlike lagged demand variables, which are used to capture operational changes in a building, lagged temperature variables are used to capture thermal inertia. Thermal inertia is residual heat energy that remains in a building after a run of warm weather (or conversely for cold weather). For example, if several warm days occur sequentially, the expected demand can increase as more cooling is typically required to maintain indoor environment quality. Hence, lagged temperature variables are based off all days - not just working days. Monday's lagged temperature variables will include temperatures observed over the weekend.


(ref:wh-line-cap) Normalised electricity demand of six commercial office buildings in Australia on `r format(fcst_test_dates[5], "%e %B, %Y")`. Only one day of data is shown although it is enough to see clear differences in the demand profiles. Both the magnitude and volatility of demand varies greatly between buildings.

```{r wh-line, fig.cap="(ref:wh-line-cap)"}
main_df %>%
  filter(date(ts) == fcst_test_dates[5]) %>%
  filter(bid %in% c("BID1501", "BID1701", "BID0220", "BID0203", "BID0128", "BID2408")) %>%
  ggplot(aes(x = ts, y = wh)) +
  geom_line(colour = ba_palette[1]) +
  facet_wrap(~bid, scales = "free_y") +
  labs(
    y = expression(Normalised ~ electricity ~ (Wh / m^2)),
    x = "Date"
  ) +
  scale_x_datetime(date_labels = "%H:%M")
```

(ref:temp-wh-scatter) Relationship between temperature and normalised electricity demand at midday and midnight for two Australian office buildings during Summer. Logged values are shown as we use these as our response variable when fitting models to enforce a positivity constraint. The relationship between temperature and demand is different both between buildings and at different times of the day.

```{r temp-wh-scatter, fig.cap="(ref:temp-wh-scatter)"}
main_df %>%
  filter(
    bid %in% c("BID0105", "BID0127"),
    month(ts) %in% c(12, 1, 2),
    period %in% c(48, 96)
  ) %>%
  mutate(time = seconds_to_period(period*15*60),
         time = sprintf("%02d:%02d", time$hour, time$minute)) %>%
  ggplot(aes(x = temperature, y = log(wh), colour = time)) +
  geom_point() +
  stat_smooth(geom='line', alpha=1, se=FALSE, method = "lm",
              formula = y ~ ns(x, df = 3)) +
  labs(
    y = expression(Log ~ normalised ~ electricity ~ (log(Wh / m^2))),
    x = "Temperature (°C)",
    colour = "Time of day"
  ) +
  facet_wrap(~bid) +
  theme(legend.position = "bottom") +
  scale_colour_manual(values=ba_palette[c(3,1)])
```


## Attribute data

Building attribute data describes different characteristics of each building. The data are Boolean and indicate if a particular attribute is absent or present. A previous paper [@Roach2020-ch] that examined important drivers of commercial office building demand identified the following attributes as relevant:

* tenant feed
* DX system
* electric element heating
* centralised distribution.

For a more detailed discussion of each of these attributes refer to @Roach2020-ch.

# Methodology

Several linear and mixed effects models were tested to determine which produced the most accurate forecasts conditional on selected features. Here we describe the various benchmarking and mixed effects models and their formulations. All analysis was produced using the R statistical programming language [@R-base]. Mixed effects models were fit using the `lme4` package for mixed effects models [@R-lme4].

## Model formulation

To justify our final model we test several models that can be thought of as simpler versions. Each model has a change introduced and the improvement in performance is used as justification for each. As a starting benchmark, we fit a naive model that uses the previous day's observed values. The second model fit individual linear regression models to each building. The third model also involves fitting a set of individual models but incorporate natural splines to model the temperature and demand relationships. The fourth model is the first to be trained using data from all the buildings and treats each building as a dummy variable. The fifth, sixth and seventh models are random intercept, random slope and subject-specific curve models. Finally, the eighth model is a subject-specific curve model that includes building attributes as fixed effects. This is summarised in Table \@ref(tab:model-frameworks).

(ref:model-frameworks) Model descriptions

```{r model-frameworks}
read_csv("data/model-descriptions.csv") %>% 
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    caption = "(ref:model-frameworks)",
    longtable = FALSE
  ) %>%
  kableExtra::kable_styling(full_width = TRUE)
```

Due to the evolving nature of energy demand across the day we fit separate models for each 15-minute period of the day. This gives 96 models for each building when fitting individual model formulations and 96 models for each mixed effects formulation.

Throughout our modelling we use natural splines to fit the relationship between predictor variables and demand. This differs somewhat from other studies on semiparametric mixed effects models which use piecewise linear splines to model variable relationships [@Durban2005-lk]. An example for temperature and electricity demand is shown in Figure \@ref(fig:plot-linear-cubic-splines). We see that a natural spline gives a more reasonable fit at the sparsely populated extremes compared to other spline functions.

(ref:linear-cubic-splines-cap) Linear and cubic splines with three degrees of freedom fit to one building's demand data during Summer at 11:45 am. We observe that the linear spline results in severe kinks in the relationship which seems unrealistic. The cubic spline gives a much smoother fit and appears to be the more reasonable option. The natural spline is better again as it does not have the dramatic dip in predicted demand that the cubic spline has for high temperatures.

```{r plot-linear-cubic-splines, fig.cap="(ref:linear-cubic-splines-cap)"}
p_data <- main_df %>%
  filter(
    bid == "BID0005",
    period == 48,
    # period %in% 40:52,
    month(ts) %in% c(12, 1, 2)
  )

lm_ls <- lm(log(wh) ~ bs(temperature, df = 3, degree = 1), p_data)
lm_cs <- lm(log(wh) ~ bs(temperature, df = 3), p_data)
lm_ns <- lm(log(wh) ~ ns(temperature, df = 3), p_data)

p_data <- p_data %>%
  mutate(
    `Linear spline` = predict(lm_ls, .),
    `Cubic spline` = predict(lm_cs, .),
    `Natural spline` = predict(lm_ns, .)
  )

ggplot(mapping = aes(x = temperature)) +
  geom_point(data = p_data, aes(y = log(wh)), shape = "O", alpha = 0.5) +
  geom_line(
    data = p_data %>%
      select(
        temperature, `Linear spline`, `Cubic spline`,
        `Natural spline`
      ) %>%
      gather(var, val, -temperature) %>%
      mutate(var = factor(var, levels = c("Linear spline", "Cubic spline", "Natural spline"))),
    aes(y = val), colour = ba_palette[1]
  ) +
  facet_wrap(~var) +
  labs(
    colour = "Method",
    y = expression(Log ~ normalised ~ electricity ~ (log(Wh / m^2))),
    x = "Temperature (°C)"
  ) +
  theme(legend.position = "none")
```

Predictors are centered and scaled prior to training models. The exact features that are used for models are determined through our feature selection approach (Section \@ref(feature-selection)). Models are fit by maximising the log-likelihood criterion.

### Individual models

Individual models serve as benchmark models to determine if moving to a mixed models framework improves prediction accuracy. Separate models are fit for each building. Note that a subscript for building has been omitted from each of these individual models to improve clarity.

#### Naive forecast model

The simplest benchmark is a naive forecasting model, where the previous day's values are used. This is often a surprisingly effective forecasting approach [@Hyndman2018-ok]. The demand of a building at time $t$ is given by
$$
  y_{t} = y_{t-24 \text{ hours}} + \epsilon_{t}, \quad \epsilon_{t} \sim N(0,\sigma^2).
$$
Note that since we have restricted ourselves to business days, $t-24 \text{ hours}$ is a slight abuse of notation and is used to represent the observed values from the last _business_ day. So a forecast for Monday will use observed values from the previous Friday. Using observed values from Sunday would produce a much weaker benchmark due to different demand dynamics on working and non-working days.

#### Individual linear regression model

A simple benchmark model is created by fitting a linear regression model to each building and period of the day. The demand of a building at time $t$ is given by
$$
\log y_{t} = \beta_{0,p} + \beta_{1,p} w_{0,t} + \epsilon_{t}, \quad \epsilon_{t} \sim N(0,\sigma^2_{p}),
$$
where $p$ is the 15-minute period of the day at time $t$, $w_{0,t}$ is the scaled temperature experienced^[The 0 subscript denotes no lag and is consistent with Table \@ref(tab:weather-vars) with the building subscript dropped.] at time $t$ and $\epsilon_{t}$ is the residual. We call these our "Individual Linear Regression" (ILR) models.

#### Individual natural spline model

A linear relationship between temperature and electricity demand may not be sufficient to adequately capture the relationship between the two. Natural cubic splines allow a more flexible relationship between predictors and the response. In this model the log demand of each building is modelled separately using natural splines. A building's demand based on temperature and other selected predictors is given by
$$
\begin{gathered}
\log y_{t} = f_p(w_{0,t}) + \epsilon_{t}, \quad \epsilon_{t} \sim N(0, \sigma^2_{p}), \\
f_p(x) = \sum^K_{k=1} \beta_{p,k} (x - \kappa_k)_+^3,
\end{gathered}
$$
where $f_p$ is a smooth function modelling the relationship between $w_{0,t}$ and the logged demand for period $p$. We refer to this set of models as the "Individual Natural Spline" (INS) models.

We use natural splines with three degrees of freedom as our smooth functions. Knot positions $\kappa_k$ are calculated based on quantiles of the data. Natural splines are chosen over other types as they enforce the constraint of linearity beyond the boundary points, which seems a fair assumption when considering the behaviour of electricity demand consumption in relation to extreme temperatures (see Figure \@ref(fig:plot-linear-cubic-splines) for an illustration). We wish to create a parsimonious model and assuming anything beyond a linear relationship in the extremes seems contrary to that aim. Failing to enforce the linearity constraint may result in unusual relationships being predicted if extrapolating beyond the training data.


\begin{table}[t]
\caption{\label{tab:weather-vars} Predictor variables evaluated during feature selection when determining $\mathcal{P}_t$.}

\centering
\begin{tabular}{lrl}
\toprule
Variable $x_{b,i,t}$ & Lag (15-minute periods) & Description \\
\midrule
$w_{b,0,t}$ & 0 & Scaled current temperature. \\
$w_{b,12,t}$ & 12 & Scaled temperature lagged by 3 hours. \\
$w_{b,24,t}$ & 24 & Scaled temperature lagged by 6 hours. \\
$w_{b,48,t}$ & 48 & Scaled temperature lagged by 12 hours. \\
$w_{b,96,t}$ & 96 & Scaled temperature lagged by 24 hours. \\
$w_{b,192,t}$ & 192 & Scaled temperature lagged by 2 days. \\
$w_{b,288,t}$ & 288 & Scaled temperature lagged by 3 days. \\
$w^+_{b,t}$ & & Maximum scaled temperature over last 24 hours. \\
$w^-_{b,t}$ & & Minimum scaled temperature over last 24 hours. \\
$\bar{w}_{b,t}$ & & Average scaled temperature over last 3 days. \\
$y_{b,96,t}$ & 96 & Scaled actual demand lagged by 1 day. \\
$y_{b,192,t}$ & 192 & Scaled actual demand lagged by 2 days. \\
$y_{b,672,t}$ & 672 & Scaled actual demand lagged by 1 week. \\
\bottomrule
\end{tabular}
\end{table}


### Pooled regression model

Our pooled regression model is fit using data from all buildings. One model is fit for each 15-minute period of the day which is then used to predict demand of each building $b$ at time $t$. Note that since all buildings are included in the model, we introduce the $b$ subscript for buildings.

Additional predictor variables are introduced in this model, such as lagged temperature variables; maximum, minimum and average temperatures; and lagged demand. A description of each predictor is presented in Table \@ref(tab:weather-vars). We denote this set of predictor variables as $\mathcal{P}_t$, which contains the selected variables for the 15-minute period of day and month at time $t$. The exact combination of variables is chosen via our feature selection methodology described in Section \@ref(feature-selection). This model is used when selecting features as it is much faster to train than a mixed effects model.

The demand of building $b$ at time $t$ is given by
$$
\begin{gathered}
\log y_{b,t} = \sum_{x_{b,i,t} \in \mathcal{P}_t } f_{i,p}(x_{b,i,t}) + \alpha_{b,p}+ \epsilon_{b,t}, \quad \epsilon_{b,t} \sim N(0, \sigma^2_{p}), \\
f_{i,p}(x) = \sum^K_{k=1} \beta_{i,p,k} (x - \kappa_k)_+^3,
\end{gathered}
$$
where $x_{b,i,t}$ is the value of building $b$'s $i$^th^ predictor variable at time $t$ and $f_{i,p}$ is a smooth function modelling the relationship between $x_{b,i,t}$ and the logged demand for period $p$. A dummy variable $\alpha_{b,p}$ has been added to account for differences in each building's consumption. We call this our "Pooled Regression" (PR) model.

We do not estimate a separate smooth relationship between weather variables and demand for each building in the pooled model. Instead, we estimate the population's relationship. So, for the $i$^th^ predictor we construct a smooth function $f_{i,p}$ for all buildings instead of a set of smooth functions $f_{b,i,p}$ for each building.

### Mixed models

Having specified our framework for fitting separate models to each building it is now time to explore fitting mixed models. In each mixed effects model that follows, all buildings are included by treating each as a random effect. In section \@ref(forecasting-accuracy) we show that mixed models improve prediction accuracy and have the added benefit of allowing us to quantify the impact of building attributes on electricity demand.

Originally, random effects were incorporated into each of the lagged weather variables, but this resulted in very poor fits presumably due to the high dimensionality. Instead, as with the PR model, we model the population relationship for all selected predictor variables and allow for subject-specific differences using random intercepts, random slopes and subject-specific curves (based on current temperature).

As with the PR model, we include subscripts $b$ to denote each building. Unlike the individual formulations, which had separate models fit to each building, all buildings are used when training the mixed effects models and so we include an additional subscript to denote this. Again, to capture changing demand characteristics across the day, separate models are fit for each 15-minute period of the day giving 96 models for each mixed effects formulation.

#### Random intercept model

The simplest mixed effects model is a random intercept (RI) model. We model the log of the demand by
$$
\begin{gathered}
\log y_{b,t} = \sum_{x_{b,i,t} \in \mathcal{P}_t } f_{i,p}(x_{b,i,t}) + u_{b,p} + \epsilon_{b,t}, \\
f_{i,p}(x) = \sum^K_{k=1} \beta_{i,p,k} (x - \kappa_k)_+^3, \\
\epsilon_{b,t} \sim N(0, \sigma^2_{\epsilon, p}), \quad u_{b,p} \sim N(0, \sigma_{u,p}^2),
\end{gathered}
$$
where $u_{b,p}$ is a random effect that controls the intercept of the model. This is similar in form to the pooled regression model, with the dummy variable $\alpha_{b,p}$ replaced by the random intercept $u_{b,p}$. We don't use this model for feature selection in Section \@ref(feature-selection) as it takes much longer to fit than the pooled regression model.

#### Random intercept and slope model

Expanding on this is the random intercept and slope (RIS) model which has a random effect for both the intercept and slope of the model. We model demand by
$$
\begin{gathered}
\log y_{bt} = \sum_{x_{b,i,t} \in \mathcal{P}_t } f_{i,p}(x_{b,i,t}) + u_{b,p,1} + u_{b,p,2} w_{b,0,t} + \epsilon_{b,t}, \\
f_{i,p}(x) = \sum^K_{k=1} \beta_{i,p,k} (x - \kappa_k)_+^3, \\
\epsilon_{b,t} \sim N(0, \sigma^2_{\epsilon, p}), \quad
(u_{b,p,1}, u_{b,p,2})^T \sim N(0, \Sigma), \quad
\Sigma = \begin{bmatrix}
              \sigma^2_{u,1} & \sigma_{u,1,2} \\
              \sigma_{u,1,2} & \sigma^2_{u,2}
            \end{bmatrix}.
\end{gathered}
$$

Here we have included a random slope based on scaled current temperature, $w_{b,0,t}$. The random effects $u_{b,p,1}$ and $u_{b,p,2}$ control the subject-specific differences for intercept and slope, respectively. The matrix $\Sigma$ is a variance-covariance matrix for the random effects. It includes terms for the variance of intercepts ($\sigma^2_{u,1}$), the variance of slopes ($\sigma^2_{u,2}$) and the covariance between intercepts and slopes ($\sigma_{u,1,2}$).

#### Subject-specific curves model

As the relationship between demand and temperature can be quite nonlinear we also explore modelling the subject-specific differences in the temperature and energy relationship using splines. We call this model the subject-specific curve (SSC) model in keeping with @Durban2005-lk. Note that we have modified their model to work with natural cubic splines as this gives a better fit when modelling the temperature and electricity relationship compared to penalized linear splines (Figure \@ref(fig:plot-linear-cubic-splines)). It is given by
$$
\begin{gathered}
\log y_{bt} = \sum_{x_{b,i,t} \in \mathcal{P}_t } f_{i,p}(x_{b,i,t}) + g_{b,p}( w_{b,0,t} ) + \epsilon_{b,t}, \\
f_{i,p}(x) = \sum^K_{k=1} \beta_{i,p,k} (x - \kappa_k)_+^3, \quad g_{b,p}(x) = \sum^K_{k=1} u_{b,p,k} (x - \kappa_k)_+^3, \\
\epsilon_{b,t} \sim N(0, \sigma^2_{\epsilon, p}), \quad
u_{b,p,k} \sim N(0, \sigma^2_{u,p}).
\end{gathered}
$$

As with our other models we use natural cubic splines with three degrees of freedom for both $f_{i,p}$ and $g_{b,p}$. This model allows us to capture separate temperature and electricity relationships for each building while also including the population relationships between electricity demand and other selected predictors.

#### Subject-specific curves with attributes model

Here we introduce several new variables into our model. These variables are the set of building attributes discussed in Section \@ref(attribute-data) which we denote by $\mathcal{A}$. We treat each of these attributes as a fixed effect. We refer to this model as the subject-specific curves with attributes (SSCATTR) model.

Our model is given below
$$
\begin{gathered}
\log y_{bt} = \sum_{x_{b,i,t} \in \mathcal{P}_t } f_{i,p}(x_{b,i,t}) + g_{b,p}( w_{b,0,t} ) + \sum_{a \in \mathcal{A}} \beta_a x_{b,a} + \epsilon_{b,t}, \\
f_{i,p}(x) = \sum^K_{k=1} \beta_{i,p,k} (x - \kappa_k)_+^3, \quad g_{b,p}(x) = \sum^K_{k=1} u_{b,p,k} (x - \kappa_k)_+^3, \\
\epsilon_{b,t} \sim N(0, \sigma^2_{\epsilon, p}), \quad
u_{b,p,k} \sim N(0, \sigma^2_{u,p}).
\end{gathered}
$$
This is the same as our SSC model with the addition of the building attributes. The fixed effect $x_{b,a}$ is a Boolean variable that indicates if attribute $a$ is present for building $b$. Including building attributes as fixed effects allows one to conduct scenario analyses by predicting the expected demand with and without certain attributes present.

## Feature selection

Carrying out feature selection for such a wide range of models was a difficult problem to approach. We take the view that it is best to keep features consistent between each of the models in order to fairly compare each during the validation stage. Hence, each model's performance is conditional on the same set of predictor variables. As we don't expect the most important predictors to be changing rapidly throughout the year we only conduct feature selection for the first business day of each month. The selected predictors are then used for all business day forecasts in the month.

Table \@ref(tab:weather-vars) shows a list of demand variables that were considered for our modelling. Lagged temperature variables are used to model the impact of thermal inertia in buildings. For example, high overnight temperatures in summer may result in high demand on the following day due to the increased cooling loads required to maintain suitable indoor environment quality. The maximum and minimum temperatures from the last 24 hours are also considered, as well as the mean temperature over the previous three days. Lagged demand values of 1, 2 and 7 days are included to capture any serial correlation in the observed demand time series.

Numerous studies have already shown the link between electricity demand and current temperature [@Ben_Taieb2016-wl; @Fan2012-bs; @Roach2019-pf; @Hong2016-lo; @Hong2019-ie]. Hence, we chose to conduct feature selection conditional on the current temperature being included. There were several reasons for this:

  * Much of the literature on load forecasting already identifies the importance of current temperature in forecasting demand and we can see clear nonlinear relationships in Figure \@ref(fig:temp-wh-scatter).
  * As temperature is strongly correlated with recent values there were occasions when the current temperature would not be selected but a slightly lagged variable would be. This seemed unrealistic and was likely caused by noise in the data rather than a lagged temperature being a better predictor than actual temperature.
  * Forcing current temperature to be included reduced the number of feature combinations to search through by a factor of 2.

We use the pooled linear regression model for feature selection as it is quick to fit using OLS and allows us to model buildings by using a dummy variable for each. Using a linear model also has the advantage of allowing us to efficiently compute the leave-one-out cross-validation (LOOCV) scores using [@Seber2012-gu]

$$
\text{CV} = \frac{1}{n} \sum_{i=1}^n \left(\frac{e_i}{1-h_i}\right)^2,
$$

where $e_i$ are the residuals of the model and $h_i$ are the diagonal elements of the hat-matrix $\mathbf{H} = \mathbf{X} \left( \mathbf{X}' \mathbf{X}  \right)^{-1} \mathbf{X}'$.

Training data are comprised of business days within a window of `r fcst_train_window` days prior to the month we wish to select variables for. As our experimental setup for the validation phase involves one-day ahead forecasts, this variable selection prevents us from using any data from the future. For example, when forecasting for any date in January, only data from the months preceding January would have been used to select predictors.

We use best subset selection during feature selection. Given $p$ predictors we choose the combination of these that produce the best $R^2$ scores. Once the best model based on $R^2$ has been determined for each set of $p$ predictors, we use the LOOCV score to determine the overall best. The LOOCV score is chosen as it gives an estimate of the out of sample performance of our models. Figure \@ref(fig:fs-model-selection) shows the LOOCV scores for each predictor set of size $p$. During feature selection we chose to avoid greedy approaches such as forward or backward stepwise selection; or approaches that work systematically through lagged weather variables [@Hyndman2010-ui]. Naturally, greedy methods have computational benefits, but it is interesting to observe which features are chosen when _all_ possible predictor combinations are assessed.

A key point to note is that feature selection was done on the weather variables and _not_ the spline basis functions. Doing so would destroy the properties of a spline if only a subset of its basis functions were to be selected.

(ref:fs-model-selection) LOOCV scores (log scale) for each month. In general, the LOOCV errors initially decrease as variables are added, but begin to increase slightly at a certain point for each period.

```{r fs-model-selection, fig.cap = "(ref:fs-model-selection)"}
fs_df %>%
  select(month, period, features) %>%
  unnest(cols = c(features)) %>%
  select(month, period, n_vars, loocv) %>%
  mutate(month = month(month, label = TRUE)) %>%
  ggplot(aes(y = loocv, x = n_vars, colour = period, group = period)) +
  geom_line(alpha = 0.5) +
  facet_wrap(~month) +
  scale_colour_gradient2(low = ba_palette[1], mid = ba_palette[4],
                         high = ba_palette[2], midpoint = 48) +
  labs(
    x = "Number of variables",
    y = "LOOCV scores",
    colour = "Period"
  ) +
  scale_x_continuous(breaks=c(0,3,6,9,12)) +
  scale_y_log10()
```

It should be noted that feature selection could be further improved for the mixed effects models by proceeding with a step-wise selection process after the above process has completed for the pooled model. Features can be added or removed based on if an appropriate out of sample accuracy score improves. This allows us to benefit from the relative speed of fitting via OLS before further fine-tuning with a greedy selection algorithm.

## Validation

### Rolling origin 1-day ahead forecasts

We used a historical training period comprised of recent observations for each building. Business days from a sliding window of length `r fcst_train_window` days were selected as training data for each model. Using recent observations allows recent operational changes or trends to be captured in each model.

If, for a given 1-day ahead forecast, a building had less than `r fcst_min_train_days` days of training data present then it was removed from the forecast. This was done to accommodate buildings that had recently been included in the data set or where the data had been censored. Training a building with less than `r fcst_min_train_days` days of data sometimes resulted in severe overfitting.

### Error measures

To assess the forecasting accuracy of each of our models we use four common error metrics.

1. Mean absolute error: $\text{MAE} = \text{mean} \left( | y_t - \hat{y}_t | \right)$.
2. Mean absolute percentage error: $\text{MAPE} = \text{mean} \left( \left| \frac{100(y_t - \hat{y}_t)}{y_t} \right| \right)$.
3. Symmetric mean absolute percentage error: $\text{sMAPE} = \text{mean} \left( \frac{200 |y_t - \hat{y}_t| }{y_t + \hat{y}_t} \right)$.
4. Mean absolute scaled error: $\text{MASE} = \text{mean} \left( \left| \frac{y_t - \hat{y}_t}{ \text{mean} ( | y_t - y_{t-1} |)} \right| \right)$.

These are all well established forecasting metrics. Advantages and disadvantages of each are described in @Hyndman2006-bp.

When comparing these metrics in Section \@ref(forecasting-accuracy), we find that the SSC and SSCATTR models produce the best point forecasts. To establish that this result is statistically significant we also carry out Diebold-Mariano tests against the ILR model in Section \@ref(diebold-mariano-test).

# Results

In order to build a better understanding of how a mixed model framework improves upon fitting individual models to each building we need to assess each model's performance. To do so, we create one-day ahead ex-post forecasts and calculate the MAE, MAPE, sMAPE and MASE for each. We focus on ex-post forecasting as we wish to examine error caused by model specification and ignore errors caused by incorrect weather forecasts, as would be the case in an ex-ante forecasting scenario.


## Variables chosen via feature selection

Figure \@ref(fig:fs-alpha) show the number of times each variable is selected for all months of the year. There is a lot of variation in the selected predictors, even between adjacent 15-minute periods. By overlaying all of the months we do observe some structure. Perhaps the most noticeable characteristic is that temperature variables are selected more often during business hours, which shows the influence temperature has on demand during the day and how temperature influences occupant behaviour. Outside of these hours we see fewer temperature variables selected. Another point of interest is that during business hours, lagged demand variables are selected less often than for non-business hours. It would appear as though serial correlation in the demand time series is a more useful predictor during non-business hours than temperature. Thermal inertia does not appear to influence demand as much during non-business hours.


(ref:fs-alpha) Feature selection for all months. The number of times a variable has been selected across all 12 months is indicated by the transparency of each tile for a given period. In general, weather features are selected more often during business hours. During non-business hours, weather features are selected less often, but lagged demand variables are almost always selected.

```{r fs-alpha, fig.height = 6, fig.cap="(ref:fs-alpha)"}
fs_df %>%
  select(period, month, vars) %>%
  unnest(cols = c(vars)) %>%
  mutate(
    selected = TRUE,
    month = month(month, label = TRUE)
  ) %>%
  spread(vars, selected, fill = FALSE) %>%
  # Ensure models with no variables selected have white squares plotted
  right_join(list(
    month = unique(month(fcst_test_dates)),
    period = fcst_test_periods
  ) %>%
    cross_df() %>%
    mutate(month = month(month, label = TRUE)),
  by = c("month", "period")
  ) %>%
  gather(vars, selected, -c(period, month)) %>%
  mutate(selected = tidyr::replace_na(selected, FALSE)) %>%
  group_by(period, vars) %>%
  summarise(n_selected = sum(selected)) %>%
  ungroup() %>%
  filter(vars != "scaled_temperature") %>%
  mutate(selected = if_else(n_selected > 0, TRUE, FALSE)) %>%
  inner_join(predictor_vars) %>%
  mutate(description = factor(description, levels = predictor_vars$description,
                              ordered = TRUE),
         description = fct_rev(description)) %>%
  filter(n_selected > 0) %>% # don't want any fill for 0
  ggplot(aes(x = period, y = description, alpha = n_selected)) +
  geom_tile(fill = ba_palette[1]) +
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "right"
  ) +
  labs(
    x = "Period",
    y = NULL,
    fill = "Predictor type",
    alpha = "Selections"
  ) +
  facet_grid(predictor_type ~ ., space = "free_y", scales = "free_y") +
  scale_x_continuous(breaks = seq(8, 96, 8))
```


## Forecasting accuracy

Table \@ref(tab:accuracy-results) shows the MAE, MAPE, sMAPE and MASE for each model across the entire day, during business hours (7:00 am to 7:00 pm) and during non-business hours (7:00 pm to 7:00 am).

The Naive model has the worst forecasting accuracy. All of the benchmark and mixed effects models outperform it. Overall, the best performing model is the SSC model, closely followed by the SSCATTR model. These two models consistently outperform others across all metrics. Figure \@ref(fig:one-day-ahead-forecast) shows an example of forecasts produced from the SSC model. The actual demand values and previous day's demand values (Naive model) are also plotted. We can see that the SSC model tends to track the general shape of each profile well and does not predict erratic spikes in demand.

Given the SSC and SSCATTR models outperform each of our benchmarks it seems reasonable to conclude that forecasting with mixed effects models is a reasonable practice that should be encouraged when data are available for similar subjects.

(ref:accuracy-results) Forecasting accuracy measures for each model across the entire day, business hours (7:00 am to 7:00 pm) and non-business hours (7:00 pm to 7:00 am).

```{r accuracy-results}
format_digits <- function(x, ...) {
  x <- signif(x, ...)
  x.nchar <- map(str_extract_all(x, "[0-9]+"), ~ sum(nchar(.)))
  x.nchar <- max(unlist(x.nchar))
  x <- format(x, digits = 5)
  x
}

accuracy_df %>%
  mutate_if(is.numeric, ~ format_digits(., digits = 3)) %>% 
  group_by(working_hours) %>%
  mutate_at(
    vars(mape, smape, mae, mase),
    ~ kableExtra::cell_spec(., format = "latex",
                            bold = if_else(. == min(.), T, F))
  ) %>%
  ungroup() %>%
  select(
    Model = model, MAE = mae, MAPE = mape, sMAPE = smape,
    MASE = mase
  ) %>%
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    longtable = FALSE,
    escape = FALSE,  # allows bold formatting to work
    caption = "(ref:accuracy-results)"
  ) %>%
  kableExtra::kable_styling() %>%
  kableExtra::group_rows("All hours", 1, 8) %>%
  kableExtra::group_rows("Business hours", 9, 16) %>%
  kableExtra::group_rows("Non-business hours", 17, 24)
```

(ref:one-day-ahead-forecast-cap) One-day ahead forecasts for `r format(p_one_day_ahead_date, "%e %B, %Y")`. The Naive (yesterday's actuals) and SSC models are shown. The naive model often includes erratic spikes whereas the SSC model tends to produce a smoother profile.

```{r one-day-ahead-forecast, fig.cap="(ref:one-day-ahead-forecast-cap)"}
p_bid <- paste0("BID", c("0025", "0058", "0112", "0203", "0205", "1101"))

model_df %>%
  filter(fcst_date == p_one_day_ahead_date) %>%
  unnest(cols = c(preds)) %>%
  filter(bid %in% p_bid,
         model %in% c("Naive", "SSC")) %>%
  mutate(ts = as_datetime(fcst_date) + minutes((period - 1) * 24 * 60 / 96)) %>%
  select(ts, model, Actual = wh, pred, bid) %>%
  pivot_wider(names_from = model, values_from = pred) %>%
  pivot_longer(cols = c(Actual, Naive, SSC)) %>%
  ggplot(aes(x = ts)) +
  geom_line(aes(y = value, colour = name)) +
  facet_wrap(~bid, ncol = 2, scales = "free_y") +
  labs(
    y = expression(Normalised ~ electricity ~ (Wh / m^2)),
    x = "Date",
    colour = "Model"
  ) +
  scale_colour_manual(values = c("black", "grey", ba_palette[2])) +
  scale_x_datetime(date_labels = "%H:%M") +
  theme(legend.position = "bottom")
```

## Diebold-Mariano test

Here we perform a one-sided Diebold-Mariano test [@Diebold2002-ru] to determine if our final model (SSCATTR) is more accurate than the baseline model (ILR). We perform a test for each period of the day (Table \@ref(tab:dm-test-periods)). When the test is applied to each period of the day we see that our SSCATTR model produces forecasts that are significantly better than the ILR model.

(ref:dm-test-periods-cap) Diebold-Mariano test to compare forecast accuracy of ILR and SSCATTR models. Alternative hypothesis is SSCATTR model is more accurate than ILR model. All periods tested separately. Only every fourth model falling on the hour is shown for legibility.

```{r dm-test-periods}
model_df %>%
  filter(period %in% (1:24 * 4)) %>%
  select(fcst_date, period, preds) %>%
  # unnest(cols = c(preds)) %>%
  unnest_legacy(preds) %>%
  select(fcst_date, period, bid, model, resid) %>%
  filter(model %in% c("ILR", "SSCATTR")) %>%
  spread(model, resid) %>%
  group_by(period) %>%
  nest() %>%
  mutate(
    dm_test = map(data, ~ dm.test(.$ILR, .$SSCATTR,
      alternative = "greater", h = 1
    )),
    dm_statistic = map_dbl(dm_test, "statistic"),
    dm_p_value = map_dbl(dm_test, "p.value")
  ) %>%
  select(period, dm_statistic, dm_p_value) %>%
  mutate(significance = case_when(
    dm_p_value < 0.001 ~ "***",
    dm_p_value < 0.01 ~ "**",
    dm_p_value < 0.05 ~ "*",
    dm_p_value < 0.1 ~ ".",
    TRUE ~ ""
  )) %>%
  mutate(
    dm_p_value = if_else(dm_p_value<0.001, "< 0.001",
                         as.character(round(dm_p_value, 3))),
    dm_statistic = round(dm_statistic, 3)) %>% 
  rename(
    Period = period, `DM statistic` = dm_statistic,
    `p-value` = dm_p_value, Significance = significance,
  ) %>%
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    caption = "(ref:dm-test-periods-cap)",
    align = "rrrl"
  ) %>%
  kableExtra::kable_styling()
```


# Conclusion

This paper explores the possibility of using mixed effects models in a forecasting role. We first specified several different models. A best subset selection approach was proposed to determine which predictor variables should be used. Feature selection was carried out for each month of the year and 15-minute period of the day, which allowed us to observe how the importance of lagged temperature and demand variables changed throughout the day.

We fit models to `r n_buildings` buildings across Australia. Separate models for each building were fitted as a benchmark. The overall predictive power of several mixed effects models were assessed against this benchmark. One-day ahead forecasts were produced for business days over a year using all forecast methods. Based on the MAE, MAPE, sMAPE and MASE scores of each model the SSC and SSCATTR models performed best. We concluded that predicting electricity demand using nonlinear mixed effects models can improve forecast accuracy.


# Acknowledgements {-}

This research project was supported by funding from Buildings Alive. I would like to thank Buildings Alive for making data available and their guidance in understanding commercial building equipment and behaviour. I would also like to thank the reviewers for their thorough and constructive feedback.

This research was supported by use of the Nectar Research Cloud, a collaborative Australian research platform supported by the National Collaborative Research Infrastructure Strategy (NCRIS).

# Data Availability Statement {-}

The data that support the findings of this study are available from Buildings Alive. Restrictions apply to the availability of these data, which were used under license for this study. Data are available from the authors with the permission of Buildings Alive.

# References {-}
